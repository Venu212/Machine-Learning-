{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConversQueryTAPAS-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMi0pGEhw1SN1n2n5g/NaI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venu212/Machine-Learning-/blob/master/ConversQueryTAPAS_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYRKXJh_ATvU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOG8qJgMBZ6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIDuSk-Nx9Wf",
        "outputId": "4a9f4def-6f9d-4cee-cf20-81f78d693c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/google-research/tapas.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tapas'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 363 (delta 64), reused 82 (delta 39), pack-reused 236\u001b[K\n",
            "Receiving objects: 100% (363/363), 334.52 KiB | 632.00 KiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETWew_GqfftL"
      },
      "source": [
        "!ls -l sample_data ./tapas/tapas/utils\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou0GkqSFzang"
      },
      "source": [
        "!pip install ./tapas\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUZihhlm3JwE"
      },
      "source": [
        "## Fetch Pretrained Models form Google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sB4zpoK2vGn",
        "outputId": "b8b62cb8-d58d-40e8-95d3-0d545dd22199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! gsutil cp gs://tapas_models/2020_04_21/tapas_sqa_base.zip . && unzip tapas_sqa_base.zip\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tapas_models/2020_04_21/tapas_sqa_base.zip...\n",
            "\\ [1 files][  1.0 GiB/  1.0 GiB]   24.8 MiB/s                                   \n",
            "Operation completed over 1 objects/1.0 GiB.                                      \n",
            "Archive:  tapas_sqa_base.zip\n",
            "   creating: tapas_sqa_base/\n",
            "  inflating: tapas_sqa_base/model.ckpt.data-00000-of-00001  \n",
            "  inflating: tapas_sqa_base/model.ckpt.index  \n",
            "  inflating: tapas_sqa_base/README.txt  \n",
            "  inflating: tapas_sqa_base/vocab.txt  \n",
            "  inflating: tapas_sqa_base/bert_config.json  \n",
            "  inflating: tapas_sqa_base/model.ckpt.meta  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16SUPpyU3XEn"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEGndTGT3R-d"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import os\n",
        "import shutil\n",
        "import csv\n",
        "import pandas as pd\n",
        "import IPython\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCZpbU6I5Ovm"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgXB2xyy5q-I"
      },
      "source": [
        "\n",
        "from tapas.utils import tf_example_utils\n",
        "from tapas.protos import interaction_pb2\n",
        "from tapas.utils import number_annotation_utils\n",
        "from tapas.scripts import prediction_utils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkwwmqo9Chlf"
      },
      "source": [
        "##Load checkpoint for prediction\n",
        "The code is to create Interaction protobuf object.  interaction_pb2. which is the datastructure we use to store examples, and then call the prediction script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB1Whe2YCQXU"
      },
      "source": [
        "os.makedirs('results/sqa/tf_examples', exist_ok=True)\n",
        "os.makedirs('results/sqa/model', exist_ok=True)\n",
        "with open('results/sqa/model/checkpoint', 'w') as f:\n",
        "  f.write('model_checkpoint_path: \"model.ckpt-0\"')\n",
        "for suffix in ['.data-00000-of-00001', '.index', '.meta']:\n",
        "  shutil.copyfile(f'tapas_sqa_base/model.ckpt{suffix}', f'results/sqa/model/model.ckpt-0{suffix}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVhl4JlVNJEl"
      },
      "source": [
        "### Read pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPXkNBGsPwDq"
      },
      "source": [
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7Z3LyMBmA5"
      },
      "source": [
        "#uploaded = files.upload()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Kow9UI7rDmh"
      },
      "source": [
        "list_of_list=[[]]\n",
        "list_of_list[0] = list(dc_df.columns)\n",
        "list_of_list.extend(dc_df.values.tolist())\n",
        "#list_of_list[[]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu3Vjed2Ew_d"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFGrZe6BuF7Q"
      },
      "source": [
        "max_seq_length =512\n",
        "vocab_file = \"tapas_sqa_base/vocab.txt\"\n",
        "config = tf_example_utils.ClassifierConversionConfig(\n",
        "    vocab_file=vocab_file,\n",
        "    max_seq_length=max_seq_length,\n",
        "    max_column_id=max_seq_length,\n",
        "    max_row_id=max_seq_length,\n",
        "    strip_column_names=False,\n",
        "    add_aggregation_candidates=False,\n",
        ")\n",
        "converter = tf_example_utils.ToClassifierTensorflowExample(config)\n",
        "\n",
        "def convert_interactions_to_examples(tables_and_queries):\n",
        "  \"\"\"Calls Tapas converter to convert interaction to example.\"\"\"\n",
        "  for idx, (table, queries) in enumerate(tables_and_queries):\n",
        "    interaction = interaction_pb2.Interaction()\n",
        "    for position, query in enumerate(queries):\n",
        "      question = interaction.questions.add()\n",
        "      question.original_text = query\n",
        "      question.id = f\"{idx}-0_{position}\"\n",
        "    for header in table[0]:\n",
        "      interaction.table.columns.add().text = header\n",
        "    for line in table[1:]:\n",
        "      row = interaction.table.rows.add()\n",
        "      for cell in line:\n",
        "        row.cells.add().text = cell\n",
        "    number_annotation_utils.add_numeric_values(interaction)\n",
        "    for i in range(len(interaction.questions)):\n",
        "      try:\n",
        "        yield converter.convert(interaction, i)\n",
        "      except ValueError as e:\n",
        "        print(f\"Can't convert interaction: {interaction.id} error: {e}\")\n",
        "        \n",
        "def write_tf_example(filename, examples):\n",
        "  with tf.io.TFRecordWriter(filename) as writer:\n",
        "    for example in examples:\n",
        "      writer.write(example.SerializeToString())\n",
        "\n",
        "def predict(table_data, queries):\n",
        "  table = table_data\n",
        "  examples = convert_interactions_to_examples([(table, queries)])\n",
        "  write_tf_example(\"results/sqa/tf_examples/test.tfrecord\", examples)\n",
        "  write_tf_example(\"results/sqa/tf_examples/random-split-1-dev.tfrecord\", [])\n",
        "  \n",
        "  ! python tapas/tapas/run_task_main.py \\\n",
        "    --task=\"SQA\" \\\n",
        "    --output_dir=\"results\" \\\n",
        "    --noloop_predict \\\n",
        "    --test_batch_size={len(queries)} \\\n",
        "    --tapas_verbosity=\"ERROR\" \\\n",
        "    --compression_type= \\\n",
        "    --init_checkpoint=\"tapas_sqa_base/model.ckpt\" \\\n",
        "    --bert_config_file=\"tapas_sqa_base/bert_config.json\" \\\n",
        "    --mode=\"predict\" 2> error\n",
        "\n",
        "\n",
        "  results_path = \"results/sqa/model/test_sequence.tsv\"\n",
        "  all_coordinates = []\n",
        "  df = pd.DataFrame(table[1:], columns=table[0])\n",
        "  display(IPython.display.HTML(df.to_html(index=False)))\n",
        "  print()\n",
        "  with open(results_path) as csvfile:\n",
        "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
        "    for row in reader:\n",
        "      coordinates = prediction_utils.parse_coordinates(row[\"answer_coordinates\"])\n",
        "      all_coordinates.append(coordinates)\n",
        "      answers = ', '.join([table[row + 1][col] for row, col in coordinates])\n",
        "      position = int(row['position'])\n",
        "      print(\">\", queries[position])\n",
        "      print(answers)\n",
        "  return all_coordinates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ncd0Wh-wfrF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6am8mnzNpvw"
      },
      "source": [
        "### Download Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlWAHtVvN5KA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_svg_AZhY7z"
      },
      "source": [
        "!pip install streamlit -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6eUQkYcPv-Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5niBQq01N8OF"
      },
      "source": [
        "## Create A Streamlit application\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5JfQFUnPRr"
      },
      "source": [
        "### Check File created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42xCTHnPVF6"
      },
      "source": [
        ""
      ]
    }
  ]
}